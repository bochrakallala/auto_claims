{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Import required modules\nimport pandas as pd\nfrom pandas_profiling import ProfileReport\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-12-10T00:02:59.752865Z","iopub.execute_input":"2021-12-10T00:02:59.753306Z","iopub.status.idle":"2021-12-10T00:02:59.761723Z","shell.execute_reply.started":"2021-12-10T00:02:59.753266Z","shell.execute_reply":"2021-12-10T00:02:59.761072Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Import train and test datasets\ntrain_df = pd.read_csv('../input/auto-insurance-data/train_auto.csv')\ntest_df  = pd.read_csv('../input/auto-insurance-data/test_auto.csv')","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:02:59.762924Z","iopub.execute_input":"2021-12-10T00:02:59.763446Z","iopub.status.idle":"2021-12-10T00:02:59.838417Z","shell.execute_reply.started":"2021-12-10T00:02:59.763411Z","shell.execute_reply":"2021-12-10T00:02:59.837557Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### 1- Data Exploration","metadata":{}},{"cell_type":"code","source":"# Generate Report Using Pandas Profiling (Variables types,descriptions and distributions, missing values, correlations, ...)\ntrain_data_profile = ProfileReport(train_df)\ntrain_data_profile","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:02:59.839662Z","iopub.execute_input":"2021-12-10T00:02:59.840530Z","iopub.status.idle":"2021-12-10T00:03:41.734084Z","shell.execute_reply.started":"2021-12-10T00:02:59.840483Z","shell.execute_reply":"2021-12-10T00:03:41.733099Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### 2- Data Wrangling / Cleaning","metadata":{}},{"cell_type":"code","source":"# Remove unneeded columns\n# The target amount is supposed to be among the target variables, so we will remove it.\n# The home value is not supposed to have an impact on the car expected claims.\n# Owning a red car means having higher premiums is a myth with no research or statistics to back it up.\n# Having a red car will not have any effect on any part of the insurance. Thus we will remove this column.\n\nto_drop = ['INDEX', \n           'TARGET_AMT',\n           'HOME_VAL',\n           'RED_CAR']\ntrain_df.drop(to_drop, inplace=True, axis=1)\ntest_df.drop(to_drop, inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:03:41.736267Z","iopub.execute_input":"2021-12-10T00:03:41.736624Z","iopub.status.idle":"2021-12-10T00:03:41.748693Z","shell.execute_reply.started":"2021-12-10T00:03:41.736560Z","shell.execute_reply":"2021-12-10T00:03:41.747739Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Fix structural errors\n# Remove the '$' and ',' symbols from the amount values in the columns 'INCOME','BLUEBOOK' and 'OLDCLAIM' supposed to be numerical.\ntrain_df[['INCOME', 'BLUEBOOK', 'OLDCLAIM']] = train_df[['INCOME', 'BLUEBOOK', 'OLDCLAIM']].replace({'\\$': '', ',': ''}, regex=True).astype(float)\ntest_df[['INCOME', 'BLUEBOOK', 'OLDCLAIM']] = test_df[['INCOME', 'BLUEBOOK', 'OLDCLAIM']].replace({'\\$': '', ',': ''}, regex=True).astype(float)\n\n# Remove the '<' symbol from the 'EDUCATION' column\ntrain_df['EDUCATION'] = train_df['EDUCATION'].replace({'<': 'lower than '}, regex=True)\ntest_df['EDUCATION'] = test_df['EDUCATION'].replace({'<': 'lower than '}, regex=True)\n\n# Remove 'z_' from the categorical columns\nCAT_COL = ['PARENT1','MSTATUS', 'SEX', 'EDUCATION', 'JOB', 'CAR_USE', 'CAR_TYPE', 'REVOKED', 'URBANICITY']\ntrain_df[CAT_COL] = train_df[CAT_COL].replace({'z_': ''}, regex=True)\ntest_df[CAT_COL]  = test_df[CAT_COL].replace({'z_': ''}, regex=True)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:03:41.750136Z","iopub.execute_input":"2021-12-10T00:03:41.750406Z","iopub.status.idle":"2021-12-10T00:03:42.205386Z","shell.execute_reply.started":"2021-12-10T00:03:41.750371Z","shell.execute_reply":"2021-12-10T00:03:42.204468Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Remove missing values\n# We have a small percentage of missing cells (1.1% in total) for the following columns (see pandas profiling report above for details) : \n                # AGE (0.7%)\n                # YOJ (0.3%)\n                # INCOME (5.5%)\n                # JOB (4.6%)\n                # CAR_AGE (6.2%)\n# We will use the median imputation and use the train median of columns to impute both train and test missing values.\ntrain_df = train_df.fillna(train_df.median())\ntest_df  = test_df.fillna(train_df.median())","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:03:42.206517Z","iopub.execute_input":"2021-12-10T00:03:42.206744Z","iopub.status.idle":"2021-12-10T00:03:42.243691Z","shell.execute_reply.started":"2021-12-10T00:03:42.206715Z","shell.execute_reply":"2021-12-10T00:03:42.242627Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### 3- Feature Engineering","metadata":{"execution":{"iopub.status.busy":"2021-12-08T00:34:29.223428Z","iopub.execute_input":"2021-12-08T00:34:29.223746Z","iopub.status.idle":"2021-12-08T00:34:29.228338Z","shell.execute_reply.started":"2021-12-08T00:34:29.223712Z","shell.execute_reply":"2021-12-08T00:34:29.227235Z"}}},{"cell_type":"code","source":"# Log transform of the INCOME column\n# This transformation will help to handle skewed data and decreases the impact of the outliers.\ntrain_df['LOG_INCOME'] = (train_df['INCOME'] + 1).transform(np.log)\ntrain_df.drop(['INCOME'], inplace=True, axis=1)\ntest_df['LOG_INCOME']  = (test_df['INCOME'] + 1).transform(np.log)\ntest_df.drop(['INCOME'], inplace=True, axis=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:03:42.245168Z","iopub.execute_input":"2021-12-10T00:03:42.245524Z","iopub.status.idle":"2021-12-10T00:03:42.258066Z","shell.execute_reply.started":"2021-12-10T00:03:42.245483Z","shell.execute_reply":"2021-12-10T00:03:42.257265Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# One hot encoding for categorical variables\ntrain_df = pd.get_dummies(train_df)\ntest_df  = pd.get_dummies(test_df)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:03:42.259445Z","iopub.execute_input":"2021-12-10T00:03:42.259709Z","iopub.status.idle":"2021-12-10T00:03:42.297626Z","shell.execute_reply.started":"2021-12-10T00:03:42.259679Z","shell.execute_reply":"2021-12-10T00:03:42.296772Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"### 4- Modeling","metadata":{}},{"cell_type":"markdown","source":"##### - The prediction of the claims is intended to predict if the insured will file a claim or not. TARGET_FLAG = {0,1} is our target, with 0,1 being categorical values that reflect that the client ‘will not file a claim’ or ‘will file a claim’, respectively.\n##### - The purpose of our machine learning model is to predict the probability of claim occurrence. This algorithm is modeled on data representing a customer that has made a claim or not made a claim. Therefore, the problem can be identified as a binary classification, where a claim is a 1 and no claim made is a 0.\n##### - Different classification algorithms can be used in our case. We will test Logistic regression, Decision Tree, XGBoost and Random Forest in order to find the one with better performance. Grid searches will be conducted to find hyper-parameters that would yield optimal performance for the models\n##### - Though, it is important to mention that our data is imbalanced with a mild degree (not very problematic). Majority Class is class 0 with no claims (74%), minority class is class 1 with claims (26%) We will first try training on the true distribution. If the model works well and generalizes, we're done! If not, we will try the downsampling or upweighting technique.","metadata":{}},{"cell_type":"code","source":"# Split train and test datasets into features and target variable\nX = train_df.drop(['TARGET_FLAG'], axis=1)# Features\ny = train_df.TARGET_FLAG # Target variable\n\nX_test = test_df.drop(['TARGET_FLAG'], axis=1)# Features\ny_test = test_df.TARGET_FLAG # Target variable","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:03:42.299977Z","iopub.execute_input":"2021-12-10T00:03:42.300213Z","iopub.status.idle":"2021-12-10T00:03:42.308522Z","shell.execute_reply.started":"2021-12-10T00:03:42.300185Z","shell.execute_reply":"2021-12-10T00:03:42.307851Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Split X and y into training and evaluation sets\nX_train,X_eval,y_train,y_eval=train_test_split(X,y,test_size=0.25,random_state=0)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:03:42.309575Z","iopub.execute_input":"2021-12-10T00:03:42.309838Z","iopub.status.idle":"2021-12-10T00:03:42.319764Z","shell.execute_reply.started":"2021-12-10T00:03:42.309811Z","shell.execute_reply":"2021-12-10T00:03:42.318946Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"#### 4.1- Logistic Regression","metadata":{}},{"cell_type":"markdown","source":"##### For the binary-dependent variables, logistic regression (LR) is a suitable model for evaluating regression. LR is a statistical analysis used to describe how a binary target variable is connected to various independent features.","metadata":{}},{"cell_type":"code","source":"# Instantiate the model(using the default parameters)\nlogreg = LogisticRegression(solver='liblinear')\n# Fit the model with data\nlogreg.fit(X_train,y_train)\n# Perform prediction on the eval set\ny_pred_logreg=logreg.predict(X_eval)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:03:42.320768Z","iopub.execute_input":"2021-12-10T00:03:42.321014Z","iopub.status.idle":"2021-12-10T00:03:42.415197Z","shell.execute_reply.started":"2021-12-10T00:03:42.320986Z","shell.execute_reply":"2021-12-10T00:03:42.414297Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"#### 4.2- Decision Tree","metadata":{}},{"cell_type":"markdown","source":"##### A decision tree is a supervised learning approach used to solve classification and regression issues, but it is mostly used to solve classification issues. It is a classifier organized by the tree structure, where the internal nodes are the data variables, the branches are the decision rules, and each node is the output.","metadata":{}},{"cell_type":"code","source":"# Create Decision Tree classifer object\nclf = DecisionTreeClassifier()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:03:42.416601Z","iopub.execute_input":"2021-12-10T00:03:42.417459Z","iopub.status.idle":"2021-12-10T00:03:42.422657Z","shell.execute_reply.started":"2021-12-10T00:03:42.417407Z","shell.execute_reply":"2021-12-10T00:03:42.421597Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters Tuning\n# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [2, 3, 5, 10, 20],\n    'min_samples_leaf': [5, 10, 20, 50, 100],\n    'criterion': [\"gini\", \"entropy\"]\n}\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator=clf, \n                           param_grid=param_grid, \n                           cv=4, n_jobs=-1, verbose=1, scoring = \"accuracy\")\n# Train Decision Tree Classifer\ngrid_search.fit(X_train, y_train)\ngrid_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:03:42.425039Z","iopub.execute_input":"2021-12-10T00:03:42.425753Z","iopub.status.idle":"2021-12-10T00:03:45.507560Z","shell.execute_reply.started":"2021-12-10T00:03:42.425687Z","shell.execute_reply":"2021-12-10T00:03:45.506746Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"clf_best = grid_search.best_estimator_\n# Perform prediction on the eval set\ny_pred_clf_best = clf_best.predict(X_eval)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:03:45.508963Z","iopub.execute_input":"2021-12-10T00:03:45.509215Z","iopub.status.idle":"2021-12-10T00:03:45.515404Z","shell.execute_reply.started":"2021-12-10T00:03:45.509182Z","shell.execute_reply":"2021-12-10T00:03:45.514801Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"#### 4.3- XGBoost","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"##### XGBoost is a novel approach proposed to raise the gradient tree. It uses various decision trees to predict a result. XGBoost stands for extreme gradient boosting. A regression and classification problem learning technique optimizes a series of weak prediction models to construct a precise and accurate predictor. It is a desirable model, because it can boost weak learners. Furthermore, it can improve the insurance risk classifier’s performance by combining multiple models. Some studies showed that the XGBoost model is the best model for the prediction of claims occurrence with a small dataset (Pesantez-Narvaez et al. 2019).","metadata":{}},{"cell_type":"code","source":"# Instantiate an XGBoost classifier object\nxgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic', nthread=1)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:03:45.516377Z","iopub.execute_input":"2021-12-10T00:03:45.517000Z","iopub.status.idle":"2021-12-10T00:03:45.528353Z","shell.execute_reply.started":"2021-12-10T00:03:45.516951Z","shell.execute_reply":"2021-12-10T00:03:45.527513Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters Tuning\n# A parameter grid for XGBoost\nparam_grid = {\n        'min_child_weight': [1, 3],\n        'subsample': [0.6, 1.0],\n        'colsample_bytree': [0.6, 1.0],\n        'max_depth': [3, 4, 5]\n        }\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = xgb,\n                           param_grid = param_grid,                        \n                           cv = 4,\n                           n_jobs = -1,\n                           verbose = 1)\n# Fit the grid search to the data\ngrid_search.fit(X_train,y_train)\ngrid_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:03:45.529519Z","iopub.execute_input":"2021-12-10T00:03:45.529739Z","iopub.status.idle":"2021-12-10T00:07:03.917034Z","shell.execute_reply.started":"2021-12-10T00:03:45.529711Z","shell.execute_reply":"2021-12-10T00:07:03.916136Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"xgb_best = grid_search.best_estimator_\n# Perform prediction on the eval set\ny_pred_xgb_best = xgb_best.predict(X_eval)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:07:03.918531Z","iopub.execute_input":"2021-12-10T00:07:03.919380Z","iopub.status.idle":"2021-12-10T00:07:03.944423Z","shell.execute_reply.started":"2021-12-10T00:07:03.919316Z","shell.execute_reply":"2021-12-10T00:07:03.943746Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"#### 4.4- Random Forests","metadata":{}},{"cell_type":"markdown","source":"##### Random forests reflect a shift to the bagged decision trees that create a broad number of de-correlated trees so that predictive efficiency can be improved further. They are a very popular“off-the-shelf” learning algorithm with good predictive performance and relatively few hyper-parameters.","metadata":{}},{"cell_type":"code","source":"# Create a based model\nrfc=RandomForestClassifier()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:07:03.945607Z","iopub.execute_input":"2021-12-10T00:07:03.945795Z","iopub.status.idle":"2021-12-10T00:07:03.949187Z","shell.execute_reply.started":"2021-12-10T00:07:03.945764Z","shell.execute_reply":"2021-12-10T00:07:03.948449Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"# Hyperparameters Tuning\n# Create the parameter grid based on the results of random search \nparam_grid = [\n{'n_estimators': [10, 25], 'max_features': [5, 10], \n 'max_depth': [10, 50, None], 'bootstrap': [True, False]}\n]\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rfc, param_grid = param_grid, \n                          cv = 3, n_jobs = -1, verbose = 2)\n# Fit the grid search to the data\ngrid_search.fit(X_train,y_train)\ngrid_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:07:03.950262Z","iopub.execute_input":"2021-12-10T00:07:03.950490Z","iopub.status.idle":"2021-12-10T00:07:08.566560Z","shell.execute_reply.started":"2021-12-10T00:07:03.950463Z","shell.execute_reply":"2021-12-10T00:07:08.565696Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"rfc_best = grid_search.best_estimator_\n# Perform prediction on the eval set\ny_pred_rfc_best = rfc_best.predict(X_eval)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:07:08.567539Z","iopub.execute_input":"2021-12-10T00:07:08.567764Z","iopub.status.idle":"2021-12-10T00:07:08.587639Z","shell.execute_reply.started":"2021-12-10T00:07:08.567738Z","shell.execute_reply":"2021-12-10T00:07:08.586873Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"### 5- Models Evaluation (Performance Prediction )","metadata":{}},{"cell_type":"markdown","source":"##### There are several metrics to evaluate a classifier model and examine how well the model fits a dataset and its performance on the unseen data.\n##### Accuracy alone for a classification problem cannot always be reliable, because it can provide bias for a majority class giving high accuracy and weak accuracy for the minority class, making it less informative for predictions, especially in the case of imbalanced data.\n##### Car insurance claims are an excellent example of imbalanced data, because the majority of policyholders do not make a claim. Therefore, if accuracy is used, there would be a bias toward a no claim class. \n##### Thus, we will use other measures, such confusion matrix, precision, recall, F1-score, and AUC","metadata":{}},{"cell_type":"code","source":"# Confusion Matrix \n# Visualize the results of the model in the form of a confusion matrix using matplotlib and seaborn.\ndef confusion_matrix_plot(y_eval, y_pred):\n    cnf_matrix = metrics.confusion_matrix(y_eval, y_pred)\n    sensitivity = cnf_matrix[0,0]/(cnf_matrix[0,0]+cnf_matrix[0,1])\n    print('Sensitivity : ', sensitivity)\n    specificity = cnf_matrix[1,1]/(cnf_matrix[1,0]+cnf_matrix[1,1])\n    print('Specificity : ', specificity)\n    class_names=[0,1] # name  of classes\n    fig, ax = plt.subplots()\n    tick_marks = np.arange(len(class_names))\n    plt.xticks(tick_marks, class_names)\n    plt.yticks(tick_marks, class_names)\n    # create heatmap\n    sns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"YlGnBu\" ,fmt='g')\n    ax.xaxis.set_label_position(\"top\")\n    plt.tight_layout()\n    plt.title('Confusion matrix', y=1.1)\n    plt.ylabel('Actual label')\n    plt.xlabel('Predicted label')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:07:08.588826Z","iopub.execute_input":"2021-12-10T00:07:08.589124Z","iopub.status.idle":"2021-12-10T00:07:08.600288Z","shell.execute_reply.started":"2021-12-10T00:07:08.589091Z","shell.execute_reply":"2021-12-10T00:07:08.599394Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Confusion Matrix Evaluation Metrics\n# Evaluate the model using model evaluation metrics such as accuracy, precision,recall and F1_score.\ndef evaluation_metrics(y_eval, y_pred):\n    print(\"Accuracy:\",metrics.accuracy_score(y_eval, y_pred))\n    print(\"Precision:\",metrics.precision_score(y_eval, y_pred))\n    print(\"Recall:\",metrics.recall_score(y_eval, y_pred))\n    print(\"F1_score:\",metrics.f1_score(y_eval, y_pred))","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:07:08.601795Z","iopub.execute_input":"2021-12-10T00:07:08.602660Z","iopub.status.idle":"2021-12-10T00:07:08.609302Z","shell.execute_reply.started":"2021-12-10T00:07:08.602615Z","shell.execute_reply":"2021-12-10T00:07:08.608372Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# ROC Curve\n# Receiver Operating Characteristic(ROC) curve is a plot of the true positive rate against the false positive rate. \n# It shows the tradeoff between sensitivity and specificity.\ndef roc_curve_plot(y_pred_proba, y_eval):\n    fpr, tpr, _ = metrics.roc_curve(y_eval,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_eval, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.legend(loc=4)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:07:08.610370Z","iopub.execute_input":"2021-12-10T00:07:08.610601Z","iopub.status.idle":"2021-12-10T00:07:08.620134Z","shell.execute_reply.started":"2021-12-10T00:07:08.610573Z","shell.execute_reply":"2021-12-10T00:07:08.619516Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"print('LOGISTIC REGRESSION')\nevaluation_metrics(y_eval, y_pred_logreg)\nprint('--------------------------------')\nprint('DECISION TREE BEST')\nevaluation_metrics(y_eval, y_pred_clf_best)\nprint('--------------------------------')\nprint('XGBOOST')\nevaluation_metrics(y_eval, y_pred_xgb_best)\nprint('--------------------------------')\nprint('RANDOM FOREST')\nevaluation_metrics(y_eval, y_pred_rfc_best)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:07:08.624271Z","iopub.execute_input":"2021-12-10T00:07:08.624895Z","iopub.status.idle":"2021-12-10T00:07:08.661692Z","shell.execute_reply.started":"2021-12-10T00:07:08.624861Z","shell.execute_reply":"2021-12-10T00:07:08.660878Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"print('LOGISTIC REGRESSION')\nconfusion_matrix_plot(y_eval, y_pred_logreg)\nprint('DECISION TREE')\nconfusion_matrix_plot(y_eval, y_pred_clf_best)\nprint('XGBOOST')\nconfusion_matrix_plot(y_eval, y_pred_xgb_best)\nprint('RANDOM FOREST')\nconfusion_matrix_plot(y_eval, y_pred_rfc_best)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:07:08.662707Z","iopub.execute_input":"2021-12-10T00:07:08.662914Z","iopub.status.idle":"2021-12-10T00:07:09.718146Z","shell.execute_reply.started":"2021-12-10T00:07:08.662888Z","shell.execute_reply":"2021-12-10T00:07:09.717411Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"print('LOGISTIC REGRESSION')\ny_pred_logreg_proba = logreg.predict_proba(X_eval)[::,1]\nroc_curve_plot(y_pred_logreg_proba, y_eval)\nprint('DECISION TREE')\ny_pred_clf_best_proba = clf_best.predict_proba(X_eval)[::,1]\nroc_curve_plot(y_pred_clf_best_proba, y_eval)\nprint('XGBOOST')\ny_pred_xgb_best_proba = xgb_best.predict_proba(X_eval)[::,1]\nroc_curve_plot(y_pred_xgb_best_proba, y_eval)\nprint('RANDOM FOREST')\ny_pred_rfc_best_proba = rfc_best.predict_proba(X_eval)[::,1]\nroc_curve_plot(y_pred_rfc_best_proba, y_eval)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:07:09.719269Z","iopub.execute_input":"2021-12-10T00:07:09.719686Z","iopub.status.idle":"2021-12-10T00:07:10.498006Z","shell.execute_reply.started":"2021-12-10T00:07:09.719651Z","shell.execute_reply":"2021-12-10T00:07:10.496669Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"#### Every model  was evaluated according to a confusion matrix, precision, recall, F1-score, and AUC. Then, we compared all classifier models and we selected the XGBoost model as the best classifier.\n#### The range of accuracy values for all ML models was between 75.69% and 79.47%. XGBoost was the best model, with a high accuracy of 79.47% . The results showed that XGBoost was most likely to solve claim prediction problems correctly.\n#### XGBoost had the highest Specificity, which means that 45% of the true negative samples were correctly classified. The Sensitivity for the XGBoost model explains that 97.16% of the samples detected as positive were actually positive. It has also the highest F1 score 53%.\n#### Based on the AUC comparison of the classifiers, the XGBoost score was 0.818, which was the best.","metadata":{}},{"cell_type":"markdown","source":"### 6- Prediction on the test set","metadata":{}},{"cell_type":"code","source":"def save_predictions(model, X_test) :\n    y_pred_xgb_best_test = model.predict(X_test)\n    predictions = pd.DataFrame(y_pred_xgb_best_test)\n    test_df  = pd.read_csv('../input/auto-insurance-data/test_auto.csv')\n    predictions.index = test_df.INDEX # for comparison\n    predictions.columns = [\"TARGET_FLAG\"]\n    predictions.to_csv(\"prediction_results.csv\")","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:07:10.499303Z","iopub.execute_input":"2021-12-10T00:07:10.499530Z","iopub.status.idle":"2021-12-10T00:07:10.504276Z","shell.execute_reply.started":"2021-12-10T00:07:10.499504Z","shell.execute_reply":"2021-12-10T00:07:10.503273Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"save_predictions(xgb_best, X_test)","metadata":{"execution":{"iopub.status.busy":"2021-12-10T00:13:41.629951Z","iopub.execute_input":"2021-12-10T00:13:41.630254Z","iopub.status.idle":"2021-12-10T00:13:41.675078Z","shell.execute_reply.started":"2021-12-10T00:13:41.630222Z","shell.execute_reply":"2021-12-10T00:13:41.674287Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}